# -*- coding: utf-8 -*-
"""Yet another copy of PlanRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZnNijXuZvzDBnycXqRE_6hC47s4_j6-l
"""

# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mv1t4r5cm06619_SXYZS9M-Fz1xu8ZW0

install required libraries
"""

import time


!pip install python-dotenv
!pip install langchain
!pip install -U sentence-transformers
!pip install rank-bm25
!pip install langchain-community langchain-core
!pip install langchain_openai
# !pip install sklearn
!pip install tqdm
!pip install numpy
!pip install docx2txt
!pip install PyPDF2

"""*Import* libraries"""

import os
from langchain_community.document_loaders import PyPDFLoader
from dotenv import load_dotenv
from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
import docx2txt
from sentence_transformers import SentenceTransformer, CrossEncoder, util
from langchain_openai.chat_models import ChatOpenAI
from langchain_community.embeddings import OllamaEmbeddings
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction import _stop_words
import string
from tqdm.autonotebook import tqdm
import numpy as np
from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from PyPDF2 import PdfReader
import subprocess

!ollama serve
# !ollama pull llama3:8b
!ollama pull llama3.2:latest
# !ollama pull mixtral:8x7b
# !ollama pull mixtral:latest
# !ollama pull mistral:7b
# !ollama pull qwen2:7b
!ollama pull gemma2:9b
# !ollama pull phi3:3.8b

!curl -fsSL https://ollama.com/install.sh | sh

class Utilities:
  def fileToString(self, inFilePath):
    with open(inFilePath, 'r') as file:
      data = file.read().replace('\n', '')
    return data

  def stringToFile(self, outFilePath, content):
    with open(outFilePath, "w") as text_file:
      text_file.write(content)

  def appendStringToFile(self, outFilePath, content):
    with open(outFilePath, "a") as text_file:
      text_file.write(content)


class RagService:
  def __init__(self, model):
    self.ut = Utilities()
    #We use the Bi-Encoder to encode all passages, so that we can use it with semantic search
    self.bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')
    self.bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens
    self.top_k = 32                          #Number of passages we want to retrieve with the bi-encoder
    load_dotenv()

    #self.MODEL = "gemma2:9b"
    self.model = model
    #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality
    self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    self.ontologies_text=""
    self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=200)
    os.environ.update({'OLLAMA_HOST': '0.0.0.0'})

  # We lower case our text and remove stop-words from indexing
  def bm25_tokenizer(self, text):
      tokenized_doc = []
      for token in text.lower().split():
          token = token.strip(string.punctuation)

          if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:
              tokenized_doc.append(token)
      return tokenized_doc

  def search(self, query):
      print("Input question:", query)

      passages = self.text_splitter.split_text(self.ontologies_text)

      corpus_embeddings = self.bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)


      ##### Semantic Search #####
      # Encode the query using the bi-encoder and find potentially relevant passages
      question_embedding = self.bi_encoder.encode(query, convert_to_tensor=True)
      question_embedding = question_embedding.cpu()



      tokenized_corpus = []
      for passage in tqdm(passages):
        tokenized_corpus.append(self.bm25_tokenizer(passage))

      bm25 = BM25Okapi(tokenized_corpus)


      hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=self.top_k)
      hits = hits[0]  # Get the hits for the first query

      ##### Re-Ranking #####
      # Now, score all retrieved passages with the cross_encoder
      cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]
      cross_scores = self.cross_encoder.predict(cross_inp)

      # Sort results by the cross-encoder scores
      for idx in range(len(cross_scores)):
          hits[idx]['cross-score'] = cross_scores[idx]

      # Output of top-5 hits from re-ranker
      # print("\n-------------------------\n")
      # print("Top-3 Cross-Encoder Re-ranker hits")
      hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)
      contexts = ""
      for hit in hits[0:6]:
          # print("\t{:.3f}\t{}".format(hit['cross-score'], passages[hit['corpus_id']].replace("\n", " ")))
          contexts = contexts + passages[hit['corpus_id']]
      return contexts

  def load_ontology(self, ontologyPath):
    self.ontologies_text += self.ut.fileToString(ontologyPath)

  def handleQuestion(self, question):


    process = subprocess.Popen("ollama serve", shell=True)

    st = time.time()

    contexts1 = self.search(query = question)

    print(contexts1)

    et = time.time()
    elapsed_time = et - st
    print('Execution time:', elapsed_time, 'seconds')


    template = """
    answer the question based on the provided context.
    context:
    {}
    question : {}
    """

    # supports many more optional parameters. Hover on your `ChatOllama(...)`
    # class to view the latest available supported parameters
    if self.model.startswith("gpt"):
        llm = ChatOpenAI(openai_api_key="PASTE_YOUR_KEY", model=self.model)
    else:
        llm = ChatOllama(model=self.model)

    prompt = ChatPromptTemplate.from_template("""
    answer the question based on the provided context.
    context:
    {context}
    question : {question}
    """)


    chain = prompt | llm | StrOutputParser()

    st = time.time()

    print(f"The output for {self.model} model")
    print(chain.invoke({"context": contexts1,"question":question}))

    et = time.time()
    elapsed_time = et - st
    print('Execution time:', elapsed_time, 'seconds')

  def annotateUserStory(self, story):
    prompt_annotate = "Create semantic knowledge graph with respect to given ontologies based on user story:"+story
    self.handleQuestion(prompt_annotate)

  def answerQuestion(self, story):
    prompt_annotate = "Answer question about ontology based on user story:"+story
    self.handleQuestion(prompt_annotate)


  def annotateTemplate(self, story, template_path):
    template = self.ut.fileToString(template_path)
    prompt_annotate = "Create semantic RDF knowledge graph in XML format without any comments with respect to given ontologies and RDF template:" + template + ", based on user story:"+ story
    self.handleQuestion(prompt_annotate)


r = RagService("gpt-4o")
r.load_ontology("ProductionOntology.txt")
r.load_ontology("DomainEditorRsaOntology.txt")
r.load_ontology("ActivityFlowOntology.txt")
question1 = "Which attributes are relevant for employee definition?"
print("BEFORE")
r.answerQuestion(question1)
print("AFTER")

question2 = "Which concepts are related to order?"
print("BEFORE")
r.answerQuestion(question2)
print("AFTER")

question3 = "How employee is related to order?"
print("BEFORE")
r.answerQuestion(question3)
print("AFTER")

"""
question1 = "Name of employee is Dusan Kostic. He has id 612. He is member of department project managers. He is member of production team and his position is mechanics designer."
print("BEFORE")
r.annotateUserStory(question1)
print("AFTER")

print("BEFORE")
r.annotateTemplate(question1, "template_employee.txt")
print("AFTER")
"""

"""
question2 =  "We have a new order for C1 company and the product we want to produce is P1. The activity starts from 2025-06-01 and ends 2025-07-31."
print("BEFORE")
r.annotateUserStory(question2)
print("AFTER")

print("BEFORE")
r.annotateTemplate(question2, "template_order.txt")
print("AFTER")

question3="Add new CNC machine with inventory id: CNC_1."
print("BEFORE")
r.annotateUserStory(question3)
print("AFTER")
"""