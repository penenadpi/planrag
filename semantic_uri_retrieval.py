# -*- coding: utf-8 -*-
"""RAG_NEW_SEMANTIC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13RdPLZpcM3uuS8hKegc65jM69t2EvaPc
"""

!pip install rdflib
!pip install fais
"""
RAG pipeline for extracting relevant URIs from RDF.

Steps:
1. Load RDF into rdflib.Graph
2. Chunk RDF (group triples by subject or small subgraphs)
3. Build bi-encoder embeddings for each chunk
4. Use ANN (faiss or sklearn) to retrieve top-K candidate chunks for a query
5. Re-rank the K candidates with a Cross-Encoder
6. Extract URIs from top-M chunks and return them (with scores)

Author: ChatGPT (GPT-5 Thinking mini)
"""

from typing import List, Dict, Tuple
import rdflib
from rdflib import Graph, URIRef, Literal
import re
import numpy as np
import os

# Embedding / rerank models
from sentence_transformers import SentenceTransformer, CrossEncoder

# ANN index
try:
    import faiss
    _HAS_FAISS = True
except Exception:
    _HAS_FAISS = False
    from sklearn.neighbors import NearestNeighbors

# -------------------------
# Utilities: RDF loading
# -------------------------
def load_rdf_graph(paths: List[str]) -> Graph:
    g = rdflib.Graph()
    for p in paths:
        fmt = None
        if p.endswith(".ttl"):
            fmt = "turtle"
        elif p.endswith(".rdf") or p.endswith(".xml"):
            fmt = "xml"
        elif p.endswith(".nt"):
            fmt = "nt"
        # let rdflib sniff if none matched
        g.parse(p, format=fmt)
        print(f"Loaded {p}, triples so far: {len(g)}")
    return g

# -------------------------
# Chunking RDF
# -------------------------
def chunk_rdf_by_subject(g: Graph) -> List[Dict]:
    """
    Produce chunks keyed by subject. Each chunk:
    {
      "subject": subject_uri (str),
      "text": textual representation (labels + predicate:object lines),
      "triples": [ (s,p,o) ],
    }
    """
    chunks = []
    subjects = set([s for s in g.subjects()])
    for s in subjects:
        triples = list(g.triples((s, None, None)))
        if not triples:
            continue
        # build a small textual representation
        lines = []
        for (sub, pred, obj) in triples:
            # try to get readable labels
            pred_lbl = get_label(g, pred) or short_uri(pred)
            if isinstance(obj, URIRef):
                obj_lbl = get_label(g, obj) or short_uri(obj)
            elif isinstance(obj, Literal):
                obj_lbl = str(obj)
            else:
                obj_lbl = str(obj)
            lines.append(f"{pred_lbl}: {obj_lbl}")
        text = " ; ".join(lines)
        chunks.append({
            "subject": str(s),
            "text": text,
            "triples": triples
        })
    print(f"Created {len(chunks)} chunks (by subject).")
    return chunks

def short_uri(node):
    s = str(node)
    # try to shorten common namespace separators
    if "#" in s:
        return s.split("#")[-1]
    if "/" in s:
        return s.split("/")[-1]
    return s

def get_label(g: Graph, uri) -> str:
    """
    Return rdfs:label or skos:prefLabel or foaf:name if present (literal).
    """
    for p in [rdflib.RDFS.label, rdflib.term.URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"),
              rdflib.term.URIRef("http://xmlns.com/foaf/0.1/name")]:
        for o in g.objects(uri, p):
            if isinstance(o, Literal):
                return str(o)
    # fallback: try rdfs:label language-less
    return None

# -------------------------
# Entity resolution (simple)
# -------------------------
def resolve_entity_to_uri(g: Graph, name: str, limit=5) -> List[str]:
    """
    Naive resolution: search for labels that match the name (case-insensitive substring).
    Returns list of candidate URIs as strings.
    For production use, replace with a proper entity linker (Wikidata lookup or dedicated EL service).
    """
    name_norm = name.strip().lower()
    candidates = set()
    # search rdfs:label and other label predicates
    label_preds = [rdflib.RDFS.label,
                   rdflib.term.URIRef("http://www.w3.org/2004/02/skos/core#prefLabel"),
                   rdflib.term.URIRef("http://xmlns.com/foaf/0.1/name")]
    for pred in label_preds:
        for s, o in g.subject_objects(pred):
            try:
                if isinstance(o, Literal) and name_norm in str(o).lower():
                    candidates.add(str(s))
            except Exception:
                continue
            if len(candidates) >= limit:
                break
        if len(candidates) >= limit:
            break
    return list(candidates)

# -------------------------
# Embeddings & Index
# -------------------------
class RetrieverIndex:
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.bi_encoder = SentenceTransformer(model_name)
        self.texts = []     # texts list
        self.ids = []       # subject URIs
        self.embeddings = None
        self._faiss_index = None
        self._nn = None

    def add_chunks(self, chunks: List[Dict]):
        for c in chunks:
            self.texts.append(c["text"])
            self.ids.append(c["subject"])
        # compute embeddings
        print("Computing embeddings for chunks...")
        self.embeddings = np.array(self.bi_encoder.encode(self.texts, convert_to_numpy=True, show_progress_bar=True))
        # build index
        d = self.embeddings.shape[1]
        if _HAS_FAISS:
            print("Building FAISS index...")
            self._faiss_index = faiss.IndexFlatIP(d)  # inner product (we will normalize)
            # normalize vectors (for cosine)
            faiss.normalize_L2(self.embeddings)
            self._faiss_index.add(self.embeddings)
        else:
            print("Building sklearn NearestNeighbors index...")
            self._nn = NearestNeighbors(n_neighbors=5, metric="cosine").fit(self.embeddings)

    def query(self, query_text: str, k=10) -> List[Tuple[str, float]]:
        q_emb = np.array(self.bi_encoder.encode([query_text], convert_to_numpy=True))
        if _HAS_FAISS:
            faiss.normalize_L2(q_emb)
            D, I = self._faiss_index.search(q_emb, k)
            idxs = I[0]
            scores = D[0]
        else:
            # sklearn returns distances (cosine distances), convert to similarity
            dist, idxs = self._nn.kneighbors(q_emb, n_neighbors=k)
            idxs = idxs[0]
            dist = dist[0]
            scores = 1.0 - dist  # approximate similarity
        results = []
        for idx, score in zip(idxs, scores):
            if idx < 0 or idx >= len(self.ids):
                continue
            results.append((self.ids[idx], float(score)))
        return results

# -------------------------
# Re-rank with Cross-Encoder
# -------------------------
class Reranker:
    def __init__(self, model_name="cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.cross = CrossEncoder(model_name)

    def rerank(self, query: str, candidates: List[Dict], top_m=5) -> List[Tuple[Dict, float]]:
        """
        candidates: list of dicts with 'subject' and 'text'
        returns top_m list of (candidate_dict, score)
        """
        pair_inputs = [(query, c["text"]) for c in candidates]
        scores = self.cross.predict(pair_inputs)
        scored = list(zip(candidates, scores))
        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[:top_m]

# -------------------------
# Extraction of URIs
# -------------------------
def extract_uris_from_chunks(scored_chunks: List[Tuple[Dict, float]]) -> List[Tuple[str, float]]:
    """
    Given re-ranked chunks (candidate dict + score), produce (uri,score) list.
    We de-duplicate by subject URI.
    """
    seen = set()
    out = []
    for c, score in scored_chunks:
        subj = c["subject"]
        if subj in seen:
            continue
        seen.add(subj)
        out.append((subj, float(score)))
    return out

# -------------------------
# Putting it together: pipeline_run
# -------------------------
def pipeline_run(rdf_paths: List[str], user_query: str,
                 bi_model="all-MiniLM-L6-v2",
                 cross_model="cross-encoder/ms-marco-MiniLM-L-6-v2",
                 top_k=20, top_m=5):
    # 1. Load RDF
    g = load_rdf_graph(rdf_paths)

    # 2. (Optional) Resolve entities mentioned in query to canonical URIs
    # Simple heuristic: look for capitalized tokens or quoted phrases
    # For demonstration, do a naive label-based resolution for tokens > 2 chars
    token_candidates = re.findall(r"[A-Z][a-zA-Z0-9\-\_]+", user_query)
    resolved_uris = []
    for t in token_candidates:
        uris = resolve_entity_to_uri(g, t, limit=3)
        resolved_uris.extend(uris)
    if resolved_uris:
        print("Resolved entity URIs (candidates):", resolved_uris)

    # 3. Chunk RDF (by subject)
    chunks = chunk_rdf_by_subject(g)

    # 4. Build retriever index
    retriever = RetrieverIndex(model_name=bi_model)
    retriever.add_chunks(chunks)

    # 5. Retrieve top_k candidate URIs from bi-encoder
    retrieved = retriever.query(user_query, k=top_k)
    # transform into candidate dicts
    id_to_chunk = {c["subject"]: c for c in chunks}
    candidates = []
    for subj_uri, score in retrieved:
        # safe-guard
        chunk = id_to_chunk.get(subj_uri)
        if chunk:
            candidates.append(chunk)

    print(f"Retrieved {len(candidates)} bi-encoder candidates")

    # 6. Re-rank candidates with cross-encoder
    reranker = Reranker(model_name=cross_model)
    top_scored = reranker.rerank(user_query, candidates, top_m=top_m)
    print(f"Re-ranked top {len(top_scored)} chunks")

    # 7. Extract URIs from top chunks
    final_uris = extract_uris_from_chunks(top_scored)
    return final_uris, top_scored

# -------------------------
# Example usage
# -------------------------
if __name__ == "__main__":
    # Provide paths to your RDF files here
    rdf_files = [
        "test2.rdf"
    ]

    if not rdf_files:
        print("No RDF files provided in rdf_files list. Create or pass RDF files to test.")

    # Sample query
    query1 = "Which Research Areas are present?"

    # Run pipeline (will error if models or RDF missing)
    try:
        uris, scored = pipeline_run(rdf_files, query1, top_k=10, top_m=5)
        print("Final URIs (ranked):")
        for uri, score in uris:
            print(uri, score)
    except Exception as e:
        print("Pipeline failed (likely missing RDF files or model downloads). Error:", e)